{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/francoarenas91/Big-Data-Analytics-final-project/blob/main/exploratory_FFA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhM6Gbjl3tC_"
      },
      "outputs": [],
      "source": [
        "! pip install pyspark -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbdg6_AS4Cdo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import functions as fn\n",
        "from pyspark.sql.functions import col, when, expr, split, size, length, regexp_replace, year\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUfysN4Q4Ge6",
        "outputId": "2ef3a008-fbab-42ac-9692-0c2496139f25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-16 22:09:32--  https://files.consumerfinance.gov/ccdb/complaints.csv.zip\n",
            "Resolving files.consumerfinance.gov (files.consumerfinance.gov)... 13.226.34.117, 13.226.34.127, 13.226.34.108, ...\n",
            "Connecting to files.consumerfinance.gov (files.consumerfinance.gov)|13.226.34.117|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 751905510 (717M) [binary/octet-stream]\n",
            "Saving to: ‘complaints.csv.zip’\n",
            "\n",
            "complaints.csv.zip  100%[===================>] 717.07M   173MB/s    in 4.0s    \n",
            "\n",
            "2024-03-16 22:09:36 (179 MB/s) - ‘complaints.csv.zip’ saved [751905510/751905510]\n",
            "\n",
            "Archive:  complaints.csv.zip\n",
            "  inflating: /content/complaints.csv  \n"
          ]
        }
      ],
      "source": [
        "# get data\n",
        "!wget https://files.consumerfinance.gov/ccdb/complaints.csv.zip -O complaints.csv.zip\n",
        "!unzip -o complaints.csv.zip -d /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkMWl2ma4I6f",
        "outputId": "34e93433-fe3c-47ff-ab7f-756375a5e838"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+-----+--------+----+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n",
            "|Date received|             Product|         Sub-product|               Issue|           Sub-issue|Consumer complaint narrative|Company public response|             Company|State|ZIP code|Tags|Consumer consent provided?|Submitted via|Date sent to company|Company response to consumer|Timely response?|Consumer disputed?|Complaint ID|\n",
            "+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+-----+--------+----+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n",
            "|   2024-02-29|Credit reporting ...|    Credit reporting|Credit monitoring...|Didn't receive se...|                        NULL|                   NULL|Experian Informat...|   NM|   87121|NULL|                      NULL|          Web|          2024-02-29|                 In progress|             Yes|               N/A|     8444287|\n",
            "|   2024-02-29|Credit reporting ...|    Credit reporting|Problem with a co...|Their investigati...|                        NULL|                   NULL|Experian Informat...|   IN|   46226|NULL|                      NULL|          Web|          2024-02-29|                 In progress|             Yes|               N/A|     8444294|\n",
            "|   2024-02-29|Credit reporting ...|    Credit reporting|Problem with a co...|Difficulty submit...|                        NULL|                   NULL|Experian Informat...|   CA|   90222|NULL|                      NULL|          Web|          2024-02-29|                 In progress|             Yes|               N/A|     8444315|\n",
            "|   2024-02-29|         Credit card|General-purpose c...|Closing your account|Company closed yo...|                        NULL|                   NULL|GOLDMAN SACHS BAN...|   NY|   12305|NULL|                      NULL|          Web|          2024-02-29|        Closed with non-m...|             Yes|               N/A|     8444319|\n",
            "|   2024-02-29|Credit reporting ...|    Credit reporting|Problem with a co...|Their investigati...|                        NULL|                   NULL|Experian Informat...|   NY|   11207|NULL|                      NULL|          Web|          2024-02-29|                 In progress|             Yes|               N/A|     8444321|\n",
            "+-------------+--------------------+--------------------+--------------------+--------------------+----------------------------+-----------------------+--------------------+-----+--------+----+--------------------------+-------------+--------------------+----------------------------+----------------+------------------+------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# initialize SparkSession\n",
        "spark = SparkSession\\\n",
        "    .builder\\\n",
        "    .appName(\"Consumer Complaints\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "# read the csv file\n",
        "df = spark.read.csv(\"/content/complaints.csv\",\n",
        "                    header=True,\n",
        "                    multiLine=True,\n",
        "                    quote=\"\\\"\",\n",
        "                    escape=\"\\\"\",\n",
        "                    inferSchema=True)\n",
        "\n",
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZvLXyMC4JCr"
      },
      "outputs": [],
      "source": [
        "# define a cutoff date\n",
        "# using a static dataset ensures consistency in our model training\n",
        "cutoff_date = \"2024-01-01\"\n",
        "df = df.filter(fn.col(\"Date received\") < fn.lit(cutoff_date))\n",
        "# df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save and load downsized dataframe for quick testing"
      ],
      "metadata": {
        "id": "lZyLvoEt55e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df.limit(1000).write.parquet(\"/content/complaints_limited.parquet\",mode=\"overwrite\")\n",
        "# df=spark.read.parquet(\"/content/complaints_limited.parquet\")\n",
        "\n",
        "# df.count()"
      ],
      "metadata": {
        "id": "WyaMFzWc6AFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR_fMItO5LQe"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NXucnWN5Mvj"
      },
      "outputs": [],
      "source": [
        "df_filtered = df.filter(df[\"Consumer complaint narrative\"].isNotNull())\n",
        "df_filtered = df_filtered.dropDuplicates()\n",
        "df_filtered = df_filtered.filter(size(split(col(\"Consumer complaint narrative\"), \"\\s+\")) > 1)\n",
        "df_filtered = df_filtered.filter(size(split(col(\"Consumer complaint narrative\"), \"\\s+\")) < 800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mduAUql_5Yen"
      },
      "outputs": [],
      "source": [
        "# step 1: update \"Consumer Loan\" based on Sub-products\n",
        "df_filtered = df_filtered.withColumn(\"Product\",\n",
        "    when(\n",
        "        (col(\"Product\") == \"Consumer Loan\") & col(\"Sub-product\").isin([\"Vehicle loan\", \"Vehicle lease\"]),\n",
        "        \"Vehicle loan or lease\"\n",
        "    ).otherwise(col(\"Product\")))\n",
        "\n",
        "df_filtered = df_filtered.withColumn(\"Product\",\n",
        "    when(\n",
        "        (col(\"Product\") == \"Consumer Loan\") & ~col(\"Sub-product\").isin([\"Vehicle loan\", \"Vehicle lease\"]),\n",
        "        \"Payday loan, title loan, or personal loan\"\n",
        "    ).otherwise(col(\"Product\")))\n",
        "\n",
        "# step 2: update \"Bank account or service\" for relevant Sub-products\n",
        "df_filtered = df_filtered.withColumn(\"Product\",\n",
        "    when(\n",
        "        (col(\"Product\") == \"Bank account or service\") & col(\"Sub-product\").isin([\"Checking account\", \"Savings account\", \"(CD) Certificate of deposit\"]),\n",
        "        \"Checking or savings account\"\n",
        "    ).when(\n",
        "        (col(\"Product\") == \"Bank account or service\") & (col(\"Sub-product\") == \"Other bank product/service\"),\n",
        "        \"Other financial service\"\n",
        "    ).when(\n",
        "        (col(\"Product\") == \"Bank account or service\") & (col(\"Sub-product\") == \"Cashing a check without an account\"),\n",
        "        \"Money transfer, virtual currency, or money service\"\n",
        "    ).otherwise(col(\"Product\"))\n",
        ")\n",
        "\n",
        "# step 3: update other Product categories\n",
        "allowed_values = [\n",
        "    \"Credit reporting, credit repair services, or other personal consumer reports\",\n",
        "    \"Debt collection\",\n",
        "    \"Mortgage\",\n",
        "    \"Credit card or prepaid card\",\n",
        "    \"Checking or savings account\",\n",
        "    \"Student loan\",\n",
        "    \"Money transfer, virtual currency, or money service\",\n",
        "    \"Vehicle loan or lease\",\n",
        "    \"Payday loan, title loan, or personal loan\",\n",
        "    \"Other financial service\",\n",
        "    \"Debt or credit management\"\n",
        "]\n",
        "\n",
        "df_filtered = df_filtered.withColumn(\"Product\",\n",
        "    when(col(\"Product\").isin(allowed_values), col(\"Product\"))\n",
        "    .when(col(\"Product\") == \"Credit card\", \"Credit card or prepaid card\")\n",
        "    .when(col(\"Product\") == \"Prepaid card\", \"Credit card or prepaid card\")\n",
        "    .when(col(\"Product\") == \"Payday loan\", \"Payday loan, title loan, or personal loan\")\n",
        "    .when(col(\"Product\") == \"Payday loan, title loan, personal loan, or advance loan\", \"Payday loan, title loan, or personal loan\")\n",
        "    .when(col(\"Product\") == \"Money transfers\", \"Money transfer, virtual currency, or money service\")\n",
        "    .when(col(\"Product\") == \"Virtual currency\", \"Money transfer, virtual currency, or money service\")\n",
        "    .when(col(\"Product\") == \"Credit reporting\", \"Credit reporting, credit repair services, or other personal consumer reports\")\n",
        "    .when(col(\"Product\") == \"Credit reporting or other personal consumer reports\", \"Credit reporting, credit repair services, or other personal consumer reports\")\n",
        "    .otherwise(\"Other financial service\")\n",
        ")\n",
        "\n",
        "df_filtered = df_filtered.filter(~(df_filtered['Product'].isin([\"Other financial service\", \"Debt or credit management\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EM8EmySI5bS0"
      },
      "outputs": [],
      "source": [
        "selected_columns = ['Product', 'Sub-issue', 'Consumer complaint narrative',\n",
        "                    'Company', 'ZIP code', 'Date sent to company']\n",
        "df_selected = df_filtered.select(selected_columns).dropna()\n",
        "# df_selected.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAj3ZRy15huq"
      },
      "outputs": [],
      "source": [
        "# df_selected.groupBy(\"Product\").count().orderBy('count', ascending=False).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save cleaned dataset to file to avoid repeating the process"
      ],
      "metadata": {
        "id": "H_5sg67sEs64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_selected.write.parquet(\"/content/cleaned_dataset.parquet\",mode=\"overwrite\")\n",
        "df_selected = spark.read.parquet(\"/content/cleaned_dataset.parquet\")"
      ],
      "metadata": {
        "id": "TrFrghNREsnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkmjVtm4qTGr"
      },
      "source": [
        "# Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of top words"
      ],
      "metadata": {
        "id": "ralKK5t29AHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
        "# from pyspark.sql.functions import col, explode, desc\n",
        "# from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
        "\n",
        "# top_words=10\n",
        "\n",
        "# regexTokenizer = RegexTokenizer(inputCol=\"Consumer complaint narrative\", outputCol=\"words\", pattern=\"\\\\W+\")\n",
        "# stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "\n",
        "\n",
        "# words = regexTokenizer.transform(df_selected)\n",
        "# words= stopwordsRemover.transform(words)\n",
        "\n",
        "\n",
        "# exploded_words = words.withColumn(\"words\", explode(col(\"filtered_words\")))\n",
        "# top_words=exploded_words\\\n",
        "#                         .select(\"words\")\\\n",
        "#                         .groupBy(\"words\")\\\n",
        "#                         .count()\\\n",
        "#                         .orderBy(desc(\"count\")).limit(top_words).rdd.map(lambda row: row.words).collect()\n",
        "\n",
        "# # top_words = word_count.limit(10).select(\"words\").rdd.map(lambda row: row.word).collect()\n",
        "# top_words\n"
      ],
      "metadata": {
        "id": "OLgPyGpNoMyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vectorize the columns with IDF\n"
      ],
      "metadata": {
        "id": "Ky4bA5ZC845N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(inputCol=\"Consumer complaint narrative\", outputCol=\"words\")\n",
        "\n",
        "# Remove stop words\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
        "\n",
        "# Vectorize text\n",
        "cv = CountVectorizer(inputCol=\"filtered\",\n",
        "                     outputCol=\"rawFeatures\",\n",
        "                      vocabSize = 100, # the size of the vocabulary\n",
        "                     minDF = 2.0 #in how many documents must a vocabulary word appear\n",
        "                     )\n",
        "\n",
        "# Apply IDF\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"word_features\") #min doc freq ignores word that appears in less than X documents\n",
        "\n",
        "# Pipeline\n",
        "wordVecPipeline = Pipeline(stages=[tokenizer, remover, cv, idf])\n",
        "\n",
        "# Fit the pipeline to your data\n",
        "# df_vectorized = wordVecPipeline.fit(df_selected).transform(df_selected)\n",
        "\n",
        "\n",
        "# Show the vectorized text\n",
        "# df_vectorized.show()"
      ],
      "metadata": {
        "id": "O0oRZ_g39wvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: word2vect captures more nuance, but is much more computing intensive"
      ],
      "metadata": {
        "id": "4nH8BrHH_UHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # sin and cos transformation for day and month\n",
        "# from pyspark.sql.functions import col, to_date, dayofmonth, month, sin, cos\n",
        "# from math import pi\n",
        "\n",
        "# max_days_expr = when(col(\"Month\").isin(1, 3, 5, 7, 8, 10, 12), 31) \\\n",
        "#     .when(col(\"Month\").isin(4, 6, 9, 11), 30) \\\n",
        "#     .when(col(\"Month\") == 2, 28)\n",
        "\n",
        "# df_dates = df_vectorized.withColumn(\"Month\", month(col(\"Date sent to company\")))\\\n",
        "#        .withColumn(\"Day\", dayofmonth(col(\"Date sent to company\")))\\\n",
        "#        .withColumn(\"NormalizedDay\", col(\"Day\") / max_days_expr)\\\n",
        "#        .withColumn(\"Month_sin\", sin(col(\"Month\") * (2 * pi) / 12)) \\\n",
        "#        .withColumn(\"Month_cos\", cos(col(\"Month\") * (2 * pi) / 12)) \\\n",
        "#        .withColumn(\"Day_sin\", sin(col(\"NormalizedDay\") * 2 * pi)) \\\n",
        "#        .withColumn(\"Day_cos\", cos(col(\"NormalizedDay\") * 2 * pi))\n",
        "\n",
        "# # df_dates.show()"
      ],
      "metadata": {
        "id": "xbD4Tc0UIxm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create custom transformer for this date encoding"
      ],
      "metadata": {
        "id": "tY5TG5OyHSwg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import keyword_only\n",
        "from pyspark.ml import Transformer\n",
        "from pyspark.ml.param.shared import Param, Params\n",
        "from pyspark.sql.functions import col, month, dayofmonth, sin, cos, when\n",
        "from math import pi\n",
        "\n",
        "class DateFeatureTransformer(Transformer, Params):\n",
        "    \"\"\"\n",
        "    A custom Transformer that adds date-related features to a DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    @keyword_only\n",
        "    def __init__(self, inputCol=None, outputCols=None):\n",
        "        super(DateFeatureTransformer, self).__init__()\n",
        "        self.inputCol = Param(self, \"inputCol\", \"\")\n",
        "        self.outputCols = Param(self, \"outputCols\", \"\")\n",
        "        self._setDefault(inputCol=inputCol, outputCols=outputCols)\n",
        "\n",
        "        if inputCol is not None:\n",
        "            self.setInputCol(inputCol)\n",
        "        if outputCols is not None:\n",
        "            self.setOutputCols(outputCols)\n",
        "\n",
        "    def setInputCol(self, value):\n",
        "        \"\"\"\n",
        "        Sets the value of inputCol.\n",
        "        \"\"\"\n",
        "        return self._set(inputCol=value)\n",
        "\n",
        "    def setOutputCols(self, value):\n",
        "        \"\"\"\n",
        "        Sets the value of outputCols.\n",
        "        \"\"\"\n",
        "        return self._set(outputCols=value)\n",
        "\n",
        "    def getInputCol(self):\n",
        "        \"\"\"\n",
        "        Gets the value of inputCol or its default value.\n",
        "        \"\"\"\n",
        "        return self.getOrDefault(self.inputCol)\n",
        "\n",
        "    def getOutputCols(self):\n",
        "        \"\"\"\n",
        "        Gets the value of outputCols or its default value.\n",
        "        \"\"\"\n",
        "        return self.getOrDefault(self.outputCols)\n",
        "\n",
        "    def _transform(self, df):\n",
        "        # Define the expression for calculating the maximum number of days in a month\n",
        "        max_days_expr = when(col(\"Month\").isin(1, 3, 5, 7, 8, 10, 12), 31) \\\n",
        "            .when(col(\"Month\").isin(4, 6, 9, 11), 30) \\\n",
        "            .when(col(\"Month\") == 2, 28)\n",
        "\n",
        "        # Apply transformations\n",
        "        df_transformed = df.withColumn(\"Month\", month(col(self.getInputCol()))) \\\n",
        "            .withColumn(\"Day\", dayofmonth(col(self.getInputCol()))) \\\n",
        "            .withColumn(\"NormalizedDay\", col(\"Day\") / max_days_expr) \\\n",
        "            .withColumn(\"Month_sin\", sin(col(\"Month\") * (2 * pi) / 12)) \\\n",
        "            .withColumn(\"Month_cos\", cos(col(\"Month\") * (2 * pi) / 12)) \\\n",
        "            .withColumn(\"Day_sin\", sin(col(\"NormalizedDay\") * 2 * pi)) \\\n",
        "            .withColumn(\"Day_cos\", cos(col(\"NormalizedDay\") * 2 * pi))\n",
        "\n",
        "        return df_transformed\n",
        "\n",
        "date_transf = DateFeatureTransformer(inputCol=\"Date sent to company\",\n",
        "                                     outputCols=[\"Month\", \"Day\", \"NormalizedDay\", \"Month_sin\", \"Month_cos\", \"Day_sin\", \"Day_cos\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "tBQrJSerHXtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One hot encode company"
      ],
      "metadata": {
        "id": "wBtZSZ35Aymv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "\n",
        "string_indexer = StringIndexer(inputCol = 'Company',\n",
        "                               outputCol = 'Company_indexed',\n",
        "                               handleInvalid = 'skip')\n",
        "\n",
        "ohe = OneHotEncoder(inputCol = 'Company_indexed',\n",
        "                    outputCol = 'Company_ohe')\n",
        "\n",
        "ohePipeline = Pipeline(stages=[string_indexer,ohe])\n",
        "\n",
        "# df_ohe = ohePipeline.fit(df_dates).transform(df_dates)\n",
        "\n",
        "# df_ohe.show()"
      ],
      "metadata": {
        "id": "FxfGWu97A415"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assemble all inputs"
      ],
      "metadata": {
        "id": "NvfTChzcCKku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "#no need to scale as everithing is normalized already\n",
        "vector_assembler = VectorAssembler(\n",
        "    inputCols = ['word_features', 'Month_sin', 'Month_cos', 'Day_sin','Day_cos',\"Company_ohe\"],\n",
        "    outputCol = 'features')\n",
        "\n",
        "\n",
        "# df_model=vector_assembler.transform(df_binary)\n",
        "\n",
        "# df_model.show()"
      ],
      "metadata": {
        "id": "Z4VltBSiCKDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Putting all transformations together in a pipeline and text-train split to avoid data leakage"
      ],
      "metadata": {
        "id": "e85-ARpNGl2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_binary = df_selected.withColumn(\"binary_y\",\n",
        "                              when(col(\"product\") == \"Credit reporting, credit repair services, or other personal consumer reports\", 1).otherwise(0))\n",
        "\n",
        "train_df, test_df = df_binary.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "pipeline=Pipeline(stages=[tokenizer,remover,cv,idf, date_transf, string_indexer,ohe,vector_assembler])\n",
        "\n",
        "feature_eng= pipeline.fit(train_df)\n",
        "train_transformed = feature_eng.transform(train_df)\n",
        "test_transformed = feature_eng.transform(test_df)\n"
      ],
      "metadata": {
        "id": "pg0BwW89GuSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save and read to file to avoid re-executing all the featue engineering in the test df"
      ],
      "metadata": {
        "id": "jITrBJwmOALz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transformed.write.parquet(\"/content/train_transformed.parquet\",mode=\"overwrite\")\n",
        "train_transformed = spark.read.parquet(\"/content/train_transformed.parquet\")\n",
        "\n",
        "test_transformed.write.parquet(\"/content/test_transformed.parquet\",mode=\"overwrite\")\n",
        "test_transformed = spark.read.parquet(\"/content/test_transformed.parquet\")"
      ],
      "metadata": {
        "id": "QaoHsXriOI6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try random forest"
      ],
      "metadata": {
        "id": "yiHmLafuM80P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "rf = RandomForestClassifier(featuresCol=\"features\",\n",
        "                            labelCol=\"binary_y\",\n",
        "                            numTrees=20,\n",
        "                            maxDepth=4,\n",
        "                            maxBins=32)\n",
        "\n",
        "rf_model = rf.fit(train_transformed)\n",
        "\n",
        "train_predictions = rf_model.transform(train_transformed)\n",
        "\n",
        "test_predictions = rf_model.transform(test_transformed)\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"binary_y\")\n",
        "\n",
        "train_roc_auc = evaluator.evaluate(train_predictions)\n",
        "test_roc_auc = evaluator.evaluate(test_predictions)\n",
        "print(f\"Train ROC AUC: {train_roc_auc}\")\n",
        "print(f\"Test ROC AUC: {test_roc_auc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdkOtHW9M8nG",
        "outputId": "e7df1eff-dd8e-42cf-9fe0-b23427e6afea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train ROC AUC: 0.8951796292680188\n",
            "Test ROC AUC: 0.8945488017632424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fw2518-lXsuO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGfh4s7MlRLDln1n4+RMAY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}